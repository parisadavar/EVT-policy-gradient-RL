{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import genpareto\nfrom scipy.interpolate import interp1d\nimport warnings\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\n\nad_quantiles = np.loadtxt('/kaggle/input/adquantiles/ADQuantiles.csv', delimiter=\",\",\n                          dtype=float, skiprows=1, usecols=range(1, 1000))\nad_pvals = np.round(np.linspace(0.999, 0.001, 1000), 3)  # col names\nad_shape = np.round(np.linspace(-0.5, 1, 151), 2)  # row names\n\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef var_sa(x, alph):\n    return np.sort(x)[int(np.floor(alph*len(x)))]\n\n\ndef cvar_sa(x, alph):\n    q = var_sa(x, alph)\n    y = x[x >= q]\n    return np.mean(y), q, y\n\n\ndef get_excesses(x, tp):\n    thresh = var_sa(x, tp)\n    excesses = x[x > thresh] - thresh\n    return thresh, excesses\n\n\ndef gpd_fit(y):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        xi_mle, _, sig_mle = genpareto.fit(y, floc=0)\n    return xi_mle, sig_mle\n\n\ndef gpd_ad(x, tp):\n    u, y = get_excesses(x, tp)\n    xi, sigma = gpd_fit(y)\n    z = genpareto.cdf(y, xi, 0, sigma)\n    z = np.sort(z)\n    n = len(z)\n    i = np.linspace(1, n, n)\n    stat = -n - (1/n) * np.sum((2 * i - 1) * (np.log(z) + np.log1p(-z[::-1])))\n    return u, stat, xi, sigma\n\n\ndef ad_pvalue(stat, xi):\n    row = np.where(ad_shape == max(round(xi, 2), -0.5))[0][0]\n    if stat > ad_quantiles[row, -1]:\n        xdat = ad_quantiles[row, 950:999]\n        ydat = -np.log(ad_pvals[950:999])\n        lfit = np.polyfit(xdat, ydat, 1)\n        m = lfit[0]\n        b = lfit[1]\n        p = np.exp(-(m*stat+b))\n    else:\n        bound_idx = min(np.where(stat < ad_quantiles[row, ])[0])\n        bound = ad_pvals[bound_idx]\n        if bound == 0.999:\n            p = bound\n        else:\n            x1 = ad_quantiles[row, bound_idx-1]\n            x2 = ad_quantiles[row, bound_idx]\n            y1 = -np.log(ad_pvals[bound_idx-1])\n            y2 = -np.log(ad_pvals[bound_idx])\n            lfit = interp1d([x1, x2], [y1, y2])\n            p = np.exp(-lfit(stat))\n    return p\n\n\ndef forward_stop(pvals, signif):\n    pvals_transformed = np.cumsum(-np.log(1-pvals))/np.arange(1,len(pvals)+1)\n    kf = np.where(pvals_transformed <= signif)[0]\n    if len(kf) == 0:\n        stop = 0\n    else:\n        stop = max(kf) + 1\n    if stop == pvals.size:\n        stop -= 1\n    return kf, stop\n\n\ndef cvar_evt(alph, u, xi, sigma, tp):\n    s = (1-tp)/(1-alph)\n    if xi == 0:\n        return u + sigma * (np.log(s) + 1)\n    else:\n        return u + (sigma/(1-xi)) * (1+((s**xi) - 1)/xi)\n\n\ndef cvar_ad(x, alph, tp_start=0.79, tp_end=0.98, tp_num=20, signif=0.1, cutoff=0.9):\n    tps = np.linspace(tp_start, tp_end, tp_num)\n    ad_tests = []\n    pvals = []\n    n_rejected = 0\n    \n    for tp in tps:\n        u, stat, xi, sigma = gpd_ad(x, tp)\n        if xi <= cutoff:\n            ad_tests.append([u, xi, sigma, tp])\n            pvals.append(ad_pvalue(stat, xi))\n        else:\n             n_rejected += 1\n\n    if len(ad_tests) == 0:\n        return cvar_sa(x, alph)[0], np.nan, n_rejected, xi, sigma, u\n\n    ad_tests = np.asarray(ad_tests)\n    pvals = np.asarray(pvals)\n\n    kf, stop = forward_stop(pvals, signif)\n\n    params = ad_tests[stop]\n    tp = params[-1]\n    sigma = params[-2]\n    xi = params[-3]\n    u = params[-4]\n    cvar = cvar_evt(alph, *params)\n\n    return cvar, tp, n_rejected , xi, sigma, u\n\ndef cvar_ad_same(x, alph, t):\n\n    if pd.isnull(t) == True:\n\n        return [cvar_sa(x, alph)[0]]\n    \n    else:\n        ad_tests1 = []\n        u1, stat1, xi1, sigma1 = gpd_ad(x, t)\n        ad_tests1.append([u1, xi1, sigma1, t])\n        ad_tests1 = np.asarray(ad_tests1)\n        params1 = ad_tests1[0]\n        tp3 = params1[-1]\n        sigma3 = params1[-2]\n        xi3 = params1[-3]\n        u3 = params1[-4]\n        cvar3 = cvar_evt(alph, *params1)\n        \n        return cvar3, tp3 , xi3, sigma3, u3\n\n\ndef cvar_iter(x, alph, sampsizes, cvar_fn):\n    cvars = []\n    for n in sampsizes:\n        c = cvar_fn(x[:n], alph)\n        cvars.append(c)\n    return np.array(cvars)\n\ndef cvar_iter_sa(x, alph, sampsizes, cvar_fn):\n    cvars2 = []\n    for n in sampsizes:\n        c2 = cvar_fn(x[:n], alph)\n        cvars2.append(c2)\n    return cvars2\n\ndef cvar_iter_same(x, alph, sampsizes, cvar_fn, t):\n    cvars3 = []\n    for n, m in zip(sampsizes, range(len(sampsizes))):\n        c3 = cvar_fn(x[:n], alph, t[m])\n        cvars3.append(c3)\n    return np.array(cvars3)\n\n\n# generate CVaR estimates from sample data\ndef get_cvars_evt(dist_data, alph, sampsizes):\n    n_cpus = mp.cpu_count()\n    pool = mp.Pool(n_cpus)\n    cvars_evt = [] # extreme value theory estimates\n    tp = [] # threshold percentiles chosen\n\n    for d in dist_data:\n        # evt cvar\n        result1 = [pool.apply_async(cvar_iter, args=(x, alph, sampsizes, cvar_ad)) for x in d]\n\n        # evt cvar\n        c_est1 = np.array([r.get() for r in result1])\n        #print('c_est1', c_est1)\n        cvars_evt.append(c_est1[:,:,0])\n        tp.append(c_est1[:,:,1])\n        \n    return np.array([cvars_evt, tp])\n\n\ndef get_cvars_evt_sameu(dist_data, alph, sampsizes, tp_selected):\n    n_cpus = mp.cpu_count()\n    pool = mp.Pool(n_cpus)\n    cvars_evt_same = [] # extreme value theory estimates\n\n    for d , tps_c in zip(dist_data, tp_selected):\n        # evt cvar\n        result3 = [pool.apply_async(cvar_iter_same, args=(x, alph, sampsizes, cvar_ad_same, t)) for x,t in zip(d,tps_c)]\n        c_est3 = np.array([r.get() for r in result3])\n        cvars_evt_same.append(c_est3[:,:,0])\n\n\n    return np.array([cvars_evt_same])\n\n\ndef get_cvars_sa(dist_data, alph, sampsizes):\n    n_cpus1 = mp.cpu_count()\n    pool1 = mp.Pool(n_cpus1)\n    cvars_sa = [] # sample average estimates\n    \n    for d in dist_data:\n        # sa cvar\n        result2 = [pool1.apply_async(cvar_iter_sa, args=(x, alph, sampsizes, cvar_sa)) for x in d]\n        c_est2 = np.array([r.get() for r in result2])\n    \n        cvars_sa.append(c_est2[:,:,0])\n\n    return np.array([cvars_sa])\n\n\n\ndef cvar_evt_each_epochs(shape, theta, seed):\n    \n    scale1 = ((theta-0.4)**2) +2\n\n    np.random.seed(seed)\n    pareto_data1 = np.array([genpareto.rvs(shape, loc=0, scale=scale1, size=(1, 2000))])\n    \n    scale2 = (( (theta+eps) -0.4) **2) +2\n\n    np.random.seed(seed)\n    pareto_data2 = np.array([genpareto.rvs(shape, loc=0, scale=scale2, size=(1, 2000))])\n\n    pareto_result_evt = get_cvars_evt(pareto_data1, alph, sampsizes)\n    tp_selected = pareto_result_evt[1] #tp\n\n    pareto_result_evt_sameu = get_cvars_evt_sameu(pareto_data2, alph, sampsizes, tp_selected)\n\n    return pareto_result_evt, pareto_result_evt_sameu\n\ndef cvar_sa_each_epochs(shape, theta, seed):\n    \n    scale1 = ((theta-0.4)**2) +2\n\n    np.random.seed(seed)\n    \n    pareto_data1 = np.array([genpareto.rvs(shape, loc=0, scale=scale1, size=(1, 2000))])\n\n    scale2 = (( (theta+eps) -0.4) **2) +2\n\n    np.random.seed(seed)\n    pareto_data2 = np.array([genpareto.rvs(shape, loc=0, scale=scale2, size=(1, 2000))])\n        \n    pareto_result_sa1 = get_cvars_sa(pareto_data1,alph,sampsizes)\n    \n    pareto_result_sa2 = get_cvars_sa(pareto_data2,alph,sampsizes)\n    \n    cvar_sa1 = pareto_result_sa1[0]\n    cvar_sa2 = pareto_result_sa2[0]\n\n    return cvar_sa1,cvar_sa2\n\n        \ndef cvar_true(alph,shape,sigma):\n    if shape == 0:\n        return sigma * (1 - np.ln(1-alph))\n    else:  \n        return sigma * ( ((1-alph)**(-shape))/(1-shape) + (((1-alph)**(-shape))-1)/shape)\n    \ndef cvar_derivative_true(alph,shape, theta):\n    if shape == 0:\n        return 2 * (theta-0.4) * (1 - np.ln(1-alph))\n    else:  \n        return  2 * (theta-0.4) * (((1-alph)**(-shape))/(1-shape) + (((1-alph)**(-shape))-1)/shape)\n\n    \ndef get_derivative(output1, output2):\n    \n    derivative = (output2 - output1)/eps\n    \n    return derivative\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters:\n\nshp = 0.8 #Assume the shape parameter is 0.8. You can adjust it to 0.4 or 0.6 to obtain all the results discussed in the papers.\neps = 0.01\nn_seeds = range(50)\nn_paths = range(50)\nn_iter = 500\nalph = 0.998\nsampsizes = np.linspace(2000, 3000, 1).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evt \ncvar_evt11 = []\ncvar_evt22 = []\ntt_evt = []\n\nfor j in n_paths:\n    m=0\n    v=0\n    step_size=0.01\n    beta1=0.9\n    beta2=0.999\n    epsilon=1e-8\n    initail_theta = 1\n    theta_current_evt = initail_theta\n    starting_seed = j * n_iter\n\n    tt_evt_each_path = []\n    cvar_evt11_each_path = []\n    cvar_evt22_each_path = []\n\n    for i in range(1, n_iter): \n        \n        seed_value = starting_seed + i\n        \n        pareto_result_evt_f = cvar_evt_each_epochs(shape= shp, theta= theta_current_evt, seed=seed_value) \n\n        EVT1 = pareto_result_evt_f[0] \n        EVT2 = pareto_result_evt_f[1] \n        \n        g_evt = get_derivative(EVT1[0], EVT2[0]) \n        derivative_truee_evt = cvar_derivative_true(alph=0.998,shape=shp, theta= theta_current_evt)\n        \n        g = g_evt[0][0][0]\n        \n        m = beta1*m + (1 - beta1)*g\n        v = beta2*v + (1 - beta2)*(g**2)\n\n        # assign bias corrected values to m_hat and v_hat respectively\n        m_hat = m/(1-beta1**i)\n        v_hat = v/(1-beta2**i)\n\n        theta_current_evt = theta_current_evt - step_size*(m_hat/(np.sqrt(v_hat) + epsilon))\n        tt_evt_each_path.append(theta_current_evt)\n    \n        \n        cvar_evt11_each_path.append(EVT1[0][0])\n        cvar_evt22_each_path.append(EVT2[0][0])   \n    \n    tt_evt.append(tt_evt_each_path)\n    cvar_evt11.append(cvar_evt11_each_path)\n    cvar_evt22.append(cvar_evt22_each_path)\n\n    \n    \n# sa\ncvar_sa11 = []\ncvar_sa22 = []\ntt_sa = []\n\nfor j in n_paths:\n    m=0\n    v=0\n    step_size=0.01\n    beta1=0.9\n    beta2=0.999\n    epsilon=1e-8\n    initail_theta = 1\n    theta_current_sa = initail_theta\n    \n    starting_seed = j * n_iter\n    \n    tt_sa_each_path = []\n    cvar_sa11_each_path = []\n    cvar_sa22_each_path = []\n    \n    for i in range(1, n_iter):\n\n        seed_value = starting_seed + i\n        pareto_result_sa_f = cvar_sa_each_epochs(shape= shp, theta= theta_current_sa, seed=seed_value) # output = pareto_result_evt, pareto_result_evt_sameu, pareto_cvars_sa_true1, pareto_cvars_sa_true2\n       \n        SA1 = pareto_result_sa_f[0] \n        SA2 = pareto_result_sa_f[1] \n        g_sa =  get_derivative(SA1, SA2)\n        \n        derivative_truee_sa = cvar_derivative_true(alph=0.998, shape=shp, theta= theta_current_sa)\n        \n        g = g_sa[0][0][0]\n\n        # calculate m and v\n        m = beta1*m + (1 - beta1)*g\n        v = beta2*v + (1 - beta2)*(g**2)\n\n        # assign bias corrected values to m_hat and v_hat respectively\n        m_hat = m/(1-beta1**i)\n        v_hat = v/(1-beta2**i)\n\n        theta_current_sa = theta_current_sa - step_size*(m_hat/(np.sqrt(v_hat) + epsilon))\n        \n        tt_sa_each_path.append(theta_current_sa)\n        cvar_sa11_each_path.append(SA1[0][0])\n        cvar_sa22_each_path.append(SA2[0][0])\n    \n    tt_sa.append(tt_sa_each_path)    \n    cvar_sa11.append(cvar_sa11_each_path)\n    cvar_sa22.append(cvar_sa22_each_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape CVaRs\ncvar_reshape_evt = np.reshape(cvar_evt11, (1, len(n_paths), n_iter-1))\ncvar_reshape_evt2 = np.reshape(cvar_evt22, (1, len(n_paths), n_iter-1))\ncvar_reshape_sa1 = np.reshape(cvar_sa11, (1, len(n_paths), n_iter-1))\ncvar_reshape_sa2 = np.reshape(cvar_sa22, (1, len(n_paths), n_iter-1))\n\n# reshape thetas\ntt_reshape_evt = np.reshape(tt_evt, (1, len(n_paths), n_iter-1))\ntt_reshape_sa = np.reshape(tt_sa, (1, len(n_paths), n_iter-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"True_cvars = [95.09370283178592]\nTrue_k = [0.8]\n\n\ndef rmse(x, true): \n    return np.sqrt( (np.mean((x-true)**2, axis=0).astype(float)) )\n\n\ndef cvar_rmse(cvars, cvars_true):\n    r = rmse(cvars[0], cvars_true[0])\n    return r\n\n\n# RMSE\nrmse_cvar_evt = cvar_rmse(cvar_reshape_evt, True_cvars)\nrmse_cvar_sa1 = cvar_rmse(cvar_reshape_sa1, True_cvars)\nrmse_k_evt = cvar_rmse(tt_reshape_evt, True_k)\nrmse_k_sa = cvar_rmse(tt_reshape_sa, True_k)\n\n\nplt.rcParams.update({\n    'axes.facecolor': '#f2f2f2', \n    'axes.edgecolor': 'white',\n    'axes.grid': True,\n    'grid.color': 'white'\n})\n\n# Plot RMSE K \nplt.figure(figsize=(8, 5))\nplt.plot(range(n_iter-1), rmse_k_sa, label='SA', color='blue', linestyle='-')\nplt.plot(range(n_iter-1), rmse_k_evt, label='POTPG', color='red', linestyle='-')\nplt.xlabel('Iteration')\nplt.ylabel('RMSE$_{θ}$')\nplt.legend()\nplt.tight_layout()\nplt.savefig('rmse_theta.pdf') \nplt.show()\n\n# Plot RMSE cvars \nplt.figure(figsize=(8, 5))\nplt.plot(range(n_iter-1), rmse_cvar_sa1, label='SA', color='blue', linestyle='-')\nplt.plot(range(n_iter-1), rmse_cvar_evt, label='POTPG', color='red', linestyle='-')\nplt.xlabel('Iteration')\nplt.ylabel('RMSE$_{\\widehat{J}}$')\nplt.legend()\nplt.tight_layout()\nplt.savefig('rmse_cvar.pdf') \nplt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]}]}